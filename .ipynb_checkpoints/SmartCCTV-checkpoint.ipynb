{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8372c8-b84a-4397-b620-d741e1fb5609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPORT] DETR ONNX model saved to detr-resnet50.onnx\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 170\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m dets \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39minfer(frame)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Draw boxes\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m dets:\n",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m, in \u001b[0;36mDetrOnDevice.infer\u001b[0;34m(self, frame, threshold)\u001b[0m\n\u001b[1;32m     78\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(images\u001b[38;5;241m=\u001b[39mrgb, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Run ONNX model\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m     82\u001b[0m logits, boxes \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Safe unpacking\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Compute softmax over class logits\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_keras/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:270\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    268\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39mrun(output_names, input_feed, run_options)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "import cv2\n",
    "\n",
    "\n",
    "def export_detr_to_onnx(\n",
    "    output_path: str,\n",
    "    model_name: str = \"facebook/detr-resnet-50\",\n",
    "    opset: int = 12\n",
    "):\n",
    "    \"\"\"\n",
    "    Export pretrained DETR model to ONNX format for on-device inference.\n",
    "    \"\"\"\n",
    "    # Load model & processor\n",
    "    model = DetrForObjectDetection.from_pretrained(model_name)\n",
    "    processor = DetrImageProcessor.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Create dummy input of shape (1, 3, H, W)\n",
    "    img_size = processor.size.get(\"shortest_edge\", 800)\n",
    "    dummy = torch.zeros(1, 3, img_size, img_size)\n",
    "\n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy,),\n",
    "        output_path,\n",
    "        input_names=[\"pixel_values\"],\n",
    "        output_names=[\"logits\", \"boxes\"],\n",
    "        dynamic_axes={\n",
    "            \"pixel_values\": {0: \"batch_size\"},\n",
    "            \"logits\": {0: \"batch_size\"},\n",
    "            \"boxes\": {0: \"batch_size\"}\n",
    "        },\n",
    "        opset_version=opset\n",
    "    )\n",
    "    print(f\"[EXPORT] DETR ONNX model saved to {output_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Inference on-device with ONNX Runtime\n",
    "# -----------------------------------------------------------------------------\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "class DetrOnDevice:\n",
    "    \"\"\"\n",
    "    On-device DETR inference wrapper using ONNX Runtime.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        onnx_model_path: str,\n",
    "        model_name: str = \"facebook/detr-resnet-50\",\n",
    "        provider: str = \"CPUExecutionProvider\"\n",
    "    ):\n",
    "        # Load ONNX session\n",
    "        self.session = ort.InferenceSession(\n",
    "            onnx_model_path,\n",
    "            providers=[provider]\n",
    "        )\n",
    "        # Processor for pre/post-processing\n",
    "        self.processor = DetrImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def infer(self, frame: np.ndarray, threshold: float = 0.5):\n",
    "        \"\"\"\n",
    "        Run DETR on a single BGR frame and return filtered detections.\n",
    "    \n",
    "        Args:\n",
    "            frame: BGR image as numpy array\n",
    "            threshold: confidence threshold\n",
    "        Returns:\n",
    "            List of dicts with keys: 'score', 'label', 'box' (x0,y0,x1,y1)\n",
    "        \"\"\"\n",
    "        # Resize to match ONNX input\n",
    "        resized = cv2.resize(frame, (800, 800))\n",
    "        rgb = resized[:, :, ::-1]\n",
    "        inputs = self.processor(images=rgb, return_tensors=\"np\")\n",
    "    \n",
    "        # Run ONNX model\n",
    "        outputs = self.session.run(None, {\"pixel_values\": inputs[\"pixel_values\"]})\n",
    "        logits, boxes = outputs[:2]  # Safe unpacking\n",
    "    \n",
    "        # Compute softmax over class logits\n",
    "        class_logits = logits[0, :, :-1]  # Remove 'no-object' class\n",
    "        exp_logits = np.exp(class_logits - np.max(class_logits, axis=-1, keepdims=True))  # stable softmax\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    \n",
    "        scores = np.max(probs, axis=-1)\n",
    "        labels = np.argmax(probs, axis=-1)\n",
    "        raw_boxes = boxes[0]\n",
    "    \n",
    "        # Rescale boxes to original frame size\n",
    "        h, w = frame.shape[:2]\n",
    "        bboxes = self._rescale_boxes(raw_boxes, (h, w))\n",
    "\n",
    "        detections = []\n",
    "        for score, label, box in zip(scores, labels, bboxes):\n",
    "            if score >= threshold:\n",
    "                detections.append({\n",
    "                    \"score\": float(score),\n",
    "                    \"label\": int(label),\n",
    "                    \"box\": box.tolist()\n",
    "                })\n",
    "        return self.apply_nms(detections, iou_threshold=0.5)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _rescale_boxes(boxes: np.ndarray, image_size: tuple):\n",
    "        \"\"\"\n",
    "        Convert bounding boxes from [cx,cy,w,h] normalized format to [x0,y0,x1,y1] absolute coords.\n",
    "        \"\"\"\n",
    "        h, w = image_size\n",
    "        # cx, cy, w, h are normalized relative to image dims\n",
    "        boxes_abs = boxes.copy()\n",
    "        boxes_abs[:, 0] *= w  # cx\n",
    "        boxes_abs[:, 1] *= h  # cy\n",
    "        boxes_abs[:, 2] *= w  # width\n",
    "        boxes_abs[:, 3] *= h  # height\n",
    "\n",
    "        # Convert to corner coords\n",
    "        cxcy = boxes_abs[:, :2]\n",
    "        wh = boxes_abs[:, 2:]\n",
    "        top_left = cxcy - wh / 2\n",
    "        bottom_right = cxcy + wh / 2\n",
    "        return np.hstack((top_left, bottom_right))\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_nms(detections, iou_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Apply Non-Maximum Suppression to filter overlapping boxes.\n",
    "    \n",
    "        Args:\n",
    "            detections: list of dicts with 'score', 'label', 'box'\n",
    "            iou_threshold: IoU threshold for suppression\n",
    "        Returns:\n",
    "            List of filtered detections after NMS\n",
    "        \"\"\"\n",
    "        if not detections:\n",
    "            return []\n",
    "    \n",
    "        boxes = [d['box'] for d in detections]\n",
    "        scores = [d['score'] for d in detections]\n",
    "    \n",
    "        # Convert to [x, y, w, h] for OpenCV\n",
    "        boxes_xywh = [\n",
    "            [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n",
    "            for box in boxes\n",
    "        ]\n",
    "    \n",
    "        indices = cv2.dnn.NMSBoxes(boxes_xywh, scores, score_threshold=0.5, nms_threshold=iou_threshold)\n",
    "        indices = indices.flatten() if len(indices) > 0 else []\n",
    "    \n",
    "        return [detections[i] for i in indices]\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    onnx_out = \"detr-resnet50.onnx\"\n",
    "    export_detr_to_onnx(onnx_out)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detector = DetrOnDevice(onnx_out)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        dets = detector.infer(frame)\n",
    "        # Draw boxes\n",
    "        for det in dets:\n",
    "            x0, y0, x1, y1 = map(int, det['box'])\n",
    "            cv2.rectangle(frame, (x0, y0), (x1, y1), (0,255,0), 2)\n",
    "            cv2.putText(frame, f\"{det['label']}:{det['score']:.2f}\", (x0,y0-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "        cv2.imshow(\"DETR On-Device\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df230361-8707-46de-8f40-8559e446260b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
